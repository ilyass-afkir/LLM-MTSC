# @package _global_
# python llm-erange/main.py +experiment/uea=llm_few

run_flags:
  data_understanding: false
  data_preparation: false
  run_training: true

defaults:
  - override /model: llm_few
  - override /llm: Llama-3.2-1B 

llm:
  quantization: None

training: 
  save_results_dir_path: /path/to/llm_few/results 

  use_uea: true
  use_benchmarks: false
  
  # Optimizer 
  learning_rate: 1e-4
  weight_decay: 0.01
  betas: [0.9, 0.999]
  eps: 1e-8

  # Learning Rate Scheduler
  warmup_ratio: 0.1
  alpha_f: 0.01

  # Training loop
  epochs: 100
  patience: 20
  max_grad_norm: 1.0