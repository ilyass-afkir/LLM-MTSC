# @package _global_
# python llm-erange/main.py --multirun +experiment/uea=deep_range,deep_range_v2,time_llm

run_flags:
  data_understanding: false
  data_preparation: false
  run_training: true

defaults:
  - override /model: deep_range
  - override /llm: Llama-3.2-1B 

llm:
  quantization: None

training: 
  save_results_dir_path: /path/to/deep_range_v1/results

  use_uea: true
  use_benchmarks: false
  
  # Optimizer 
  learning_rate: 1e-4
  weight_decay: 0.01
  betas: [0.9, 0.999]
  eps: 1e-8

  # Learning Rate Scheduler
  warmup_ratio: 0.1
  alpha_f: 0.01

  # Training loop
  epochs: 100
  patience: 20
  max_grad_norm: 1.0
