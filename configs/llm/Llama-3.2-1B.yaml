name: Llama-3.2-1B 
path: /path/to/Llama-3.2-1B 
quantization: "none" # "none", "4bit", "8bit"
output_attentions: True
output_hidden_states: True
hidden_size: 2048
vocab_size: 128256
num_attention_heads: 32
num_hidden_layers: 16


