name: Llama-3.2-3B 
path: /path/to/Llama-3.2-3B 
quantization: "none" # "none", "4bit", "8bit"
output_attentions: True
output_hidden_states: True
hidden_size: 3072
vocab_size: 128256
num_attention_heads: 24
num_hidden_layers: 28  
