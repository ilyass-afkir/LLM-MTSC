name: Llama-3.1-70B 
path: /path/to/Llama-3.1-70B 
quantization: "none" # "none", "4bit", "8bit"
output_attentions: True
output_hidden_states: True
hidden_size: 8192
vocab_size: 128256
num_attention_heads: 64
num_hidden_layers: 80

