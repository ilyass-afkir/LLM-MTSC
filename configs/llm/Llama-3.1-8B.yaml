name: Llama-3.1-8B 
path: /path/to/Llama-3.1-8B 
quantization: "none" # "none", "4bit", "8bit"
output_attentions: True
output_hidden_states: True
hidden_size: 4096
vocab_size: 128256
num_attention_heads: 32
num_hidden_layers: 32

